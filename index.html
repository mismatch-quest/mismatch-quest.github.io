<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Mismatch Quest">
  <meta property="og:title" content="Mismatch Quest"/>
  <meta property="og:description" content="Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"/>
  <meta property="og:url" content="https://mismatch-quest.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Mismatch Quest">
  <meta name="twitter:description" content="Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignmen">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-and-Language Benchmark, Synthetic and Compositional Images">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mismatch Quest </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">-->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/brian-gordon-38b29bb1/" target="_blank">Brian Gordon</a><sup>*</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://yonatanbitton.github.io/" target="_blank">Yonatan Bitton</a><sup>*</sup>,&nbsp;</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/yonatan-shafir-2811a3148/" target="_blank">Yonatan Shafir</a>,&nbsp;</span><br>
                    <span class="author-block">
                      <a href="https://www.roopalgarg.com/" target="_blank">Roopal Garg</a>,&nbsp;</span>
                      <span class="author-block">
                        <a href="https://research.google/people/107755/" target="_blank">Xi Chen</a>,&nbsp;</span>
                        <span class="author-block">
                          <a href="https://www.cs.huji.ac.il/~danix/" target="_blank">Dani Lischinski</a>,&nbsp;</span>
                          <span class="author-block">
                            <a href="https://danielcohenor.com/" target="_blank">Daniel Cohen-Or</a>,&nbsp;</span>
			     <span class="author-block">
                       	      <a href="https://www.linkedin.com/in/idan-szpektor-916183" target="_blank">Idan Szpektor</a>,&nbsp;</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"Tel Aviv University,&nbsp;&nbsp;</span>
                    <span class="author-block">The Hebrew University of Jerusalem,</span><br>
                    <span class="author-block">Google Research,&nbsp;&nbsp;</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>

<!--                  &lt;!&ndash; Github link &ndash;&gt;-->
<!--                  <span class="link-block">-->
<!--                    <a href="https://github.com/YOUR REPO HERE" target="_blank"-->
<!--                    class="external-link button is-normal is-rounded is-dark">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>Code</span>-->
<!--                  </a>-->
<!--                </span>-->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2303.07274" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

		  <!-- Github Link -->
	        <span class="link-block">
                  <a href="https://github.com/BrianG13/MismatchQuest" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                    </span>
                  <span>Code</span>
                </a>
              </span>

                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/mismatch-quest/SeeTRUE-Feedback" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div style="height:100px;" class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="publication-flipped">
        <div class="content has-text-justified">
            <p style="text-align:center;">
               <img src="static/images/teaser.jpg"  style="width: 70%; height: 70%"/>
               <br>
            </p>
            <h3 class="subtitle is-size-5-tablet has-text-left pb-5">
We propose enhancing traditional image-text alignment models by introducing a feedback mechanism that not only scores but also describes and visually annotates discrepancies between images and text. <span><br>
Our method utilizes large language models and visual grounding models to automatically generate a training set (TV-Feedback dataset) with plausible misaligned captions, along with textual explanations and visual indicators. <span><br>
We also provide a human-curated test set (SeeTRUE-Feedback) with authentic textual and visual misalignment annotations.<span>
<!--This paper introduces a method for explaining misalignments in text-image pairs. 
<!-- By utilizing large language models and visual grounding models, we automatically generate a training set (TV-Feedback dataset) with plausible misaligned captions, accompanied by textual explanations and visual indicators.  -->
<!--Additionally, we release a human-curated test set (SeeTRUE-Feedback) containing authentic textual and visual misalignment annotations. -->
            </h3>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

       <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Abstract
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left has-background-info-light pr-4 pl-4 pt-3 pb-3">
         <p>
        While existing image-text alignment models reach high quality binary assessments, they fall short of pinpointing the exact source of misalignment. 
	<br>In this paper, we present a method to provide detailed textual and visual explanation of detected misalignments between text-image pairs. We leverage large language models and visual grounding models to automatically construct a training set that holds plausible misaligned captions for a given image and corresponding textual explanations and visual indicators. We also publish a new human curated test set comprising ground-truth textual and visual misalignment annotations. 
	<br>Empirical results show that fine-tuning vision language models on our training set enables them to articulate misalignments and visually indicate them within images, outperforming strong baselines both on the binary alignment classification and the explanation generation tasks.
	 </p>
         </h3>
      </div>
	      
       <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           	Textual-Visual feedback on SeeTRUE-Feedback
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p> 
		Our fine-tuned model results on our generated training data.
		Please note the precision of our produced Textual-Visual feedback, which includes: a concise explanation of the misalignment, a misalignment cue that pinpoints the contradictory source in the caption, and a labeled bounding box.
	   </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/seetrue_2rows.PNG"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>

       <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           	Textual-Visual feedback ‘in-the-wild’
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p> 
		We evaluate our model’s generalization capabilities on ‘in-the-wild’ Text-to-Image (T2I) generations from , created using Adobe Firefly, Composable Diffusion, and Stable Diffusion versions 1.0 and 2.1.
	   </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/out_of_distribution.PNG"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>

       <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           	Textual-Visual Feedback Generation Pipeline (ConGen-Feedback)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p> 
For each aligned image/caption pair we define a misalignment category (object , attribute, action, or spatial relations). 
<br>Per chosen misalignment candidate, we instruct PaLM 2 API with few-shot prompts to automatically generate: (a) a contradiction caption that introduces the target misalignment; (b) a detailed explanation of the contradiction; (c) a misalignment cue that pinpoints the contradictory element in the caption; and (d) a label for the visual bounding box to be placed on the image.
<br>To create the bounding-box misalignment, we employ GroundingDINO, which takes the textual label from PaLM 2's output and places a bounding box around the corresponding element in the image. 
	   </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/generation_pipeline.PNG"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>

       <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           	TV-Feedback dataset
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p> 
We compile a set of over a million positive image-text pairs, consisting of synthetic and natural images. Approximately 65\% of our examples consist of synthetic and real images, from PickaPic, ImageReward, Flickr30k, COCO , OpenImages and ADE20K.
<br>Using our ConGen-Feedback method, we generated our training dataset to fine-tune PaLI model to predict a full Textual-Visual feedback.
	   </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/tv_feedback.PNG"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>
	      

       <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Results for Textual-Visual Feedback
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p> The Textual-Visual feedback results on SeeTRUE-Feedback (Test split) and TV-Feedback dataset (Val split). Overall, the PaLI models fine tuned on TV-Feedback dataset outperform the baselines on all metrics. 
		<br>For example, Non-PaLI models achieved Feedback NLI scores from 0.406 to 0.627, while PaLI models reached 0.718 to 0.749. 
		<br>The largest, 55B PaLI model achieved the highest performance on the binary alignment classification task. Surprisingly, it underperformed the smaller PaLI models on most feedback generation tasks. 
		<br>Specifically, the smaller PaLI 5B is best performing on the in-distribution testset, but less so on the out-of-distribution examples.
	   </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/results_table.PNG"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
<section class="hero is-small is-light">
  <div class="hero-body">
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
	      TODO
	</code></pre>
    </div>
</section>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <a href="https://www.flaticon.com/free-icons/magic" title="magic icons">Magic icons created by Freepik - Flaticon</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
  </html>
